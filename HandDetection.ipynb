{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d211fe6a-0941-44a7-be9a-44de2ec16879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83fa7ec0-40a5-4113-acd4-d173f685d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    min_detection_confidence=0.75,\n",
    "    min_tracking_confidence=0.75,\n",
    "    max_num_hands=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb46ca7-a9d1-4361-9d09-e05cc952c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/right-and-left-hand-detection-using-python/\n",
    "# https://github.com/google/mediapipe/issues/1390#issuecomment-749333655"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f93988f-dc34-4b4a-9558-fc67e688faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29400f1-31e8-4677-9941-1d1cb6054080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "  \n",
    "while True:\n",
    "    \n",
    "    # Read video frame by frame\n",
    "    success, img = cap.read()\n",
    "  \n",
    "    # Flip the image(frame)\n",
    "    img = cv2.flip(img, 1)\n",
    "  \n",
    "    # Convert BGR image to RGB image\n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  \n",
    "    # Process the RGB image\n",
    "    results = hands.process(imgRGB)\n",
    "  \n",
    "    # If hands are present in image(frame)\n",
    "    if results.multi_hand_landmarks:\n",
    "  \n",
    "        # Both Hands are present in image(frame)\n",
    "#         if len(results.multi_handedness) == 2:\n",
    "#                 # Display 'Both Hands' on the image\n",
    "#             cv2.putText(img, 'Both Hands', (250, 50),\n",
    "#                         cv2.FONT_HERSHEY_COMPLEX, 0.9,\n",
    "#                         (0, 255, 0), 2)\n",
    "            \n",
    "                    \n",
    "        for hand_landmark in results.multi_hand_landmarks:\n",
    "            x = [landmark.x for landmark in hand_landmark.landmark]\n",
    "            y = [landmark.y for landmark in hand_landmark.landmark]  \n",
    "            \n",
    "            img_height = img.shape[0]\n",
    "            img_width = img.shape[1]\n",
    "            xmin, xmax, ymin, ymax = int(min(x)*img_width), int(max(x)*img_width), int(min(y)*img_height), int(max(y)*img_height)\n",
    "\n",
    "            \n",
    "            center = np.array([np.median(x)*img_width, np.median(y)*img_height]).astype('int32')\n",
    "            cv2.circle(img, tuple(center), 10, (36, 238, 42), 1)  #for checking the center \n",
    "            cv2.rectangle(img=img, pt1=(xmin - 23, ymin - 23), pt2=(xmax + 23, ymax + 23), color=(36, 238, 42), thickness=2)\n",
    "            \n",
    "              \n",
    "    # Display Video and when 'q' is entered, destroy the window\n",
    "    cv2.imshow('Image', img)\n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166f29c-297c-435e-beac-e04488f7c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"C:\\Users\\vmoha\\AppData\\Roaming\\Python\\Python39\\site-packages\\mediapipe\\python\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f97c1-cdf5-4b69-9f30-cf0929575e87",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98b52392-ec0e-4832-bdb1-8d36d2fff513",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "  \n",
    "\n",
    "# Read video frame by frame\n",
    "success, img = cap.read()\n",
    "\n",
    "# Flip the image(frame)\n",
    "img = cv2.flip(img, 1)\n",
    "\n",
    "# Convert BGR image to RGB image\n",
    "imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Process the RGB image\n",
    "results = hands.process(imgRGB)\n",
    "\n",
    "# If hands are present in image(frame)\n",
    "if results.multi_hand_landmarks:\n",
    "\n",
    "    # Both Hands are present in image(frame)\n",
    "#         if len(results.multi_handedness) == 2:\n",
    "#                 # Display 'Both Hands' on the image\n",
    "#             cv2.putText(img, 'Both Hands', (250, 50),\n",
    "#                         cv2.FONT_HERSHEY_COMPLEX, 0.9,\n",
    "#                         (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    for hand_landmark in results.multi_hand_landmarks:\n",
    "        x = [landmark.x for landmark in hand_landmark.landmark]\n",
    "        y = [landmark.y for landmark in hand_landmark.landmark]  \n",
    "\n",
    "        img_height = img.shape[0]\n",
    "        img_width = img.shape[1]\n",
    "        xmin, xmax, ymin, ymax = int(min(x)*img_width), int(max(x)*img_width), int(min(y)*img_height), int(max(y)*img_height)\n",
    "        \n",
    "        xmin = xmin-23\n",
    "        ymin = ymin-23\n",
    "        xmax = xmax+23\n",
    "        ymax = ymax+23\n",
    "\n",
    "        center = np.array([np.median(x)*img_width, np.median(y)*img_height]).astype('int32')\n",
    "        cv2.circle(img, tuple(center), 10, (36, 238, 42), 1)  #for checking the center \n",
    "        cv2.rectangle(img=img, pt1=(xmin, ymin), pt2=(xmax, ymax), color=(36, 238, 42), thickness=2)\n",
    "        array_img = np.asarray(img)\n",
    "        array_img = array_img[ymin:ymax, xmin:xmax]\n",
    "        array_img = cv2.resize(array_img, (224,224))\n",
    "        array_img = preprocess_input(array_img)\n",
    "        \n",
    "        image_array =  []\n",
    "        image_array.append(array_img)\n",
    "        image_array = np.array(image_array)\n",
    "     \n",
    "        pred = model.predict(image_array)\n",
    "        pred = np.where(pred > 0.5, 1, 0)\n",
    "        \n",
    "   \n",
    "while True:\n",
    "    # Display Video and when 'q' is entered, destroy the window\n",
    "    cv2.imshow('Image', array_img)\n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3344fad5-d59a-4ac8-860a-6f371f0ff287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d84b539e-9dae-4a2b-9fca-63c520b5dfdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99808caf-32c2-407e-b6df-3d655703b2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc3d4a-8869-4242-a17b-e38b7c963a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
