{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d211fe6a-0941-44a7-be9a-44de2ec16879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83fa7ec0-40a5-4113-acd4-d173f685d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    min_detection_confidence=0.75,\n",
    "    min_tracking_confidence=0.75,\n",
    "    max_num_hands=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb46ca7-a9d1-4361-9d09-e05cc952c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/right-and-left-hand-detection-using-python/\n",
    "# https://github.com/google/mediapipe/issues/1390#issuecomment-749333655"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f93988f-dc34-4b4a-9558-fc67e688faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f29400f1-31e8-4677-9941-1d1cb6054080",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "letters = np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
    "'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'])\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Read video frame by frame\n",
    "    success, img = cap.read()\n",
    "  \n",
    "    # Flip the image(frame)\n",
    "    img = cv2.flip(img, 1)\n",
    "  \n",
    "    # Convert BGR image to RGB image\n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  \n",
    "    # Process the RGB image\n",
    "    results = hands.process(imgRGB)\n",
    "  \n",
    "    # If hands are present in image(frame)\n",
    "    if results.multi_hand_landmarks:\n",
    "  \n",
    "        for hand_landmark in results.multi_hand_landmarks:\n",
    "            x = [landmark.x for landmark in hand_landmark.landmark]\n",
    "            y = [landmark.y for landmark in hand_landmark.landmark]  \n",
    "            \n",
    "            img_height = img.shape[0]\n",
    "            img_width = img.shape[1]\n",
    "            xmin, xmax, ymin, ymax = int(min(x)*img_width), int(max(x)*img_width), int(min(y)*img_height), int(max(y)*img_height)\n",
    "            xmin, ymin, xmax, ymax = xmin-23, ymin-23, xmax+23, ymax+23\n",
    "\n",
    "            center = np.array([np.median(x)*img_width, np.median(y)*img_height]).astype('int32')\n",
    "            cv2.circle(img, tuple(center), 10, (36, 238, 42), 1)  #for checking the center \n",
    "            cv2.rectangle(img=img, pt1=(xmin, ymin), pt2=(xmax, ymax), color=(36, 238, 42), thickness=2)\n",
    "            \n",
    "            array_img = np.asarray(img)\n",
    "            array_img = array_img[ymin:ymax, xmin:xmax]\n",
    "            array_img = cv2.resize(array_img, (224,224))\n",
    "            array_img = preprocess_input(array_img)\n",
    "\n",
    "            image_array =  []\n",
    "            image_array.append(array_img)\n",
    "            image_array = np.array(image_array)\n",
    "\n",
    "            pred_array = model.predict(image_array)\n",
    "            \n",
    "            label = letters[np.argmax(pred_array)]\n",
    "            confidence = pred_array[0][np.argmax(pred_array)]\n",
    "            \n",
    "            if confidence > 0.4:\n",
    "                color = (0,255,0)\n",
    "                cv2.putText(img, label, (xmin-40, ymax),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.5, color, 4) \n",
    "\n",
    "                cv2.putText(img, str(confidence), (xmin-40, ymax+40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 4) \n",
    "                # time.sleep(0.001)\n",
    "\n",
    "              \n",
    "    # Display Video and when 'q' is entered, destroy the window\n",
    "    cv2.imshow('Image', img)\n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f6ce1e0-4ce0-462d-9937-0926387a8bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25912893"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_array[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166f29c-297c-435e-beac-e04488f7c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"C:\\Users\\vmoha\\AppData\\Roaming\\Python\\Python39\\site-packages\\mediapipe\\python\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f97c1-cdf5-4b69-9f30-cf0929575e87",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98b52392-ec0e-4832-bdb1-8d36d2fff513",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hands' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m imgRGB \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Process the RGB image\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mhands\u001b[49m\u001b[38;5;241m.\u001b[39mprocess(imgRGB)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# If hands are present in image(frame)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Both Hands are present in image(frame)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#                         cv2.FONT_HERSHEY_COMPLEX, 0.9,\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#                         (0, 255, 0), 2)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hands' is not defined"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "  \n",
    "\n",
    "# Read video frame by frame\n",
    "success, img = cap.read()\n",
    "\n",
    "# Flip the image(frame)\n",
    "img = cv2.flip(img, 1)\n",
    "\n",
    "# Convert BGR image to RGB image\n",
    "imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Process the RGB image\n",
    "results = hands.process(imgRGB)\n",
    "\n",
    "# If hands are present in image(frame)\n",
    "if results.multi_hand_landmarks:\n",
    "\n",
    "    # Both Hands are present in image(frame)\n",
    "#         if len(results.multi_handedness) == 2:\n",
    "#                 # Display 'Both Hands' on the image\n",
    "#             cv2.putText(img, 'Both Hands', (250, 50),\n",
    "#                         cv2.FONT_HERSHEY_COMPLEX, 0.9,\n",
    "#                         (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    for hand_landmark in results.multi_hand_landmarks:\n",
    "        x = [landmark.x for landmark in hand_landmark.landmark]\n",
    "        y = [landmark.y for landmark in hand_landmark.landmark]  \n",
    "\n",
    "        img_height = img.shape[0]\n",
    "        img_width = img.shape[1]\n",
    "        xmin, xmax, ymin, ymax = int(min(x)*img_width), int(max(x)*img_width), int(min(y)*img_height), int(max(y)*img_height)\n",
    "        \n",
    "        xmin = xmin-23\n",
    "        ymin = ymin-23\n",
    "        xmax = xmax+23\n",
    "        ymax = ymax+23\n",
    "\n",
    "        center = np.array([np.median(x)*img_width, np.median(y)*img_height]).astype('int32')\n",
    "        cv2.circle(img, tuple(center), 10, (36, 238, 42), 1)  #for checking the center \n",
    "        cv2.rectangle(img=img, pt1=(xmin, ymin), pt2=(xmax, ymax), color=(36, 238, 42), thickness=2)\n",
    "        \n",
    "        array_img = np.asarray(img)\n",
    "        array_img = array_img[ymin:ymax, xmin:xmax]\n",
    "        array_img = cv2.resize(array_img, (224,224))\n",
    "        array_img = preprocess_input(array_img)\n",
    "        \n",
    "        image_array =  []\n",
    "        image_array.append(array_img)\n",
    "        image_array = np.array(image_array)\n",
    "     \n",
    "        pred_array = model.predict(image_array)\n",
    "        letters = np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
    "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'],\n",
    "      dtype=object)\n",
    "        pred_letter = letters[np.argmax(preds_array[0])]\n",
    "        \n",
    "        \n",
    "   \n",
    "while True:\n",
    "    # Display Video and when 'q' is entered, destroy the window\n",
    "    cv2.imshow('Image', array_img)\n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3344fad5-d59a-4ac8-860a-6f371f0ff287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d84b539e-9dae-4a2b-9fca-63c520b5dfdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99808caf-32c2-407e-b6df-3d655703b2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12cc3d4a-8869-4242-a17b-e38b7c963a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
    "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'],\n",
    "      dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267d42e-a533-44d1-aa9b-7e46e5befc36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
